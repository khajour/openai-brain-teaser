{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8be20-76b2-43e6-8302-53dc4147c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "import openai\n",
    "from IPython.display import display, clear_output, Image, Audio\n",
    "\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from threading import Thread\n",
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b6779-e9c2-4f80-964c-40bd2c368c16",
   "metadata": {},
   "source": [
    "# Set Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93408e5-7619-4e30-a276-2fc4a70bc492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038016f1-2ad1-4987-ab47-d42818030bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_4V_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    "GPT_4V_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "# West US\n",
    "\n",
    "SPEECH_KEY = os.getenv(\"AZURE_SPEECH_KEY\") \n",
    "SPEECH_REGION = \"westeurope\"\n",
    "VOICE_NAME = \"JennyMultilingualV2Neural2\"\n",
    "\n",
    "PRMTS = [\"Describe this image\", \n",
    "        \"What is the number of the parking space containing the car? explain why.\",\n",
    "        \"Solve this 9 by 9 Sudoku puzzle\"]\n",
    "PROMPT_NUMBER = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10ddb9-f6ec-4dbc-b234-6a64d4bb2617",
   "metadata": {},
   "source": [
    "# Capture the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d390b2-ef1e-4a25-b677-1ff27d1e952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraCapture:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stopped = False\n",
    "        self.started = False\n",
    "        self.frame = None\n",
    "        self.screen = None\n",
    "\n",
    "    def start(self):\n",
    "        Thread(target=self.get, args=()).start()\n",
    "        return self\n",
    "    \n",
    "    def get(self):\n",
    "        while True:\n",
    "            if not self.started:\n",
    "                self.screen = display(None, display_id=True)\n",
    "                self.stream = cv2.VideoCapture(0)\n",
    "                self.started = True\n",
    "                \n",
    "            (self.grabbed, self.frame) = self.stream.read()\n",
    "            self.frame = cv2.resize(self.frame, (480, 280))\n",
    "            success, self.frame = cv2.imencode('.jpg', self.frame)\n",
    "            self.screen.update(Image(data = self.frame.tobytes()))\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stopped = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c893df-976d-4da5-938a-dcda13d4cd2c",
   "metadata": {},
   "source": [
    "# GPT-4 Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce4734f-3c5b-408e-8da1-23a84aee335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_complete(client, previous_texts, frame):\n",
    "\n",
    "    prompt_context = ' '.join(previous_texts)\n",
    "    \n",
    "    \n",
    "    prompt_message = f\"Context: {prompt_context}. {PRMTS[PROMPT_NUMBER]} \" \n",
    "\n",
    "    base64_image = base64.b64encode(frame).decode('utf-8')\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-vision-preview\",\n",
    "        max_tokens= 600,\n",
    "        n= 1,\n",
    "        messages= [\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant. Instructions: - Only answer questions related to visual brain teasers. - If you're unsure of an answer, you can say I don't know or I'm not sure and recommend users go to the IRS website for more information. \" },\n",
    "        { \"role\": \"system\", \"content\": \"Context: - At Microsoft, we're committed to the advancement of AI driven by principles that put people first. \" },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt_message\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                } ,\n",
    "           ] \n",
    "        }]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45607b-be21-4b9f-b896-a56430dbddf8",
   "metadata": {},
   "source": [
    "# Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75398c-ad30-4748-861f-bc88f43a9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speaker:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.speech_config = None\n",
    "        self.audio_config = None\n",
    "        self.speech_synthesizer = None\n",
    "\n",
    "    def start(self):\n",
    "        self.speech_config = speechsdk.SpeechConfig(subscription = SPEECH_KEY, region = SPEECH_REGION)\n",
    "        self.audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "        self.speech_config.speech_synthesis_language = \"en-US\"\n",
    "        self.speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=self.speech_config, audio_config=self.audio_config)\n",
    "        return self\n",
    "\n",
    "    def speak(self, text, voice):\n",
    "        self.speech_config.speech_synthesis_voice_name = voice\n",
    "        self.speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=self.speech_config, audio_config=self.audio_config)\n",
    "        result = self.speech_synthesizer.speak_text_async(text).get()\n",
    "        return result\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1e3a0-4973-465c-84f9-e961072bcbb7",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf1fed-1721-4502-9afa-0f60f6b23667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "camera = CameraCapture().start()\n",
    "text_speaker = Speaker().start()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = GPT_4V_KEY,\n",
    "  api_version = \"2023-12-01-preview\",\n",
    "  azure_endpoint = GPT_4V_ENDPOINT\n",
    ")\n",
    "\n",
    "previous_texts = deque(maxlen = 5)\n",
    "print(\"\")\n",
    "\n",
    "while True:\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    if camera.started:\n",
    "        input()\n",
    "        result = text_speaker.speak(PRMTS[PROMPT_NUMBER],  \"en-US-AvaNeural\")\n",
    "        PRMTS[PROMPT_NUMBER]\n",
    "        \n",
    "        try :\n",
    "            text = gpt_complete(client, previous_texts, camera.frame)\n",
    "            print(f\"\\r {timestamp}: {text}\", end=\"\")\n",
    "            result = text_speaker.speak(text, \"en-US-AndrewNeural\")\n",
    "            previous_texts.append(f\"[{timestamp}] {text}\")\n",
    "        \n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "            print( \"Please try again\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
